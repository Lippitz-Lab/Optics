\renewcommand{\lastmod}{September 18, 2023}
\renewcommand{\chapterauthors}{Markus Lippitz}

\chapter{Numerical Fourier Transformation}



\section{Discrete FT: a periodic sequence of values}


In particular, if one collects and evaluates measurement data with a computer, then one does not know the measured function $f(t)$ on a continuous axis $t$, but only at discrete times $t_k = k \, \delta t$, nor does one know the function from $t = - \infty$ to $t = + \infty$. So we have only a finite sequence of numbers $f_k$ as a starting point.
Because we do not know the sequence of numbers outside the measured interval we make the assumption that it is periodic. With $N$ measured values the period is $T = N \Delta t$. For simplicity, we also define $f_k = f_{k + N}$ and thus $f_{-k} = f_{N - k}$ with $k= 0, 1, \dots, N-1$. Thus the Fourier transform becomes\sidenote{see \cite{Butz2015} chap. 4, \cite{Horowitz_Hill}, chap. 1.08, 7.20, 15.18}
\begin{equation}
  F_j =  \frac{1}{N} \, \sum_{k=0}^{N-1} \, f_k \, e^{- k \, j \, 2 \pi i / N } 
 \end{equation}
and its inverse transform
 \begin{equation}
 f_k =   \sum_{j=0}^{N-1} \, F_j \, e^{+ k \,  j \, 2 \pi i / N } \quad .
 \end{equation}
The definition is again such that $F_0$ corresponds to the mean. Because of $f_{-k} = f_{N - k}$, the positive frequencies are in the first half of $F_j$ as the frequency increases. After that come the negative frequencies, starting at the 'most negative' frequency and increasing to the last frequency before zero. So the maximum frequency that can be represented is the Nyquist (angular) frequency
\begin{equation}
\Omega_\text{Nyquist} = \frac{\pi}{\delta t} \quad .
\end{equation}
%
This frequency is such that we take two samples per period of the
oscillation. Faster oscillations or fewer samples per period cannot be
represented. Even with $f_\text{Nyquist}$ the imaginary part is always zero, because we always sample the sine at the 
zero crossing.


% \begin{jllisting}
%   using Plots
%   x = range(0, 2 * pi; length=100)
%   plot(x, sin.(x); label="ein Sinus")
% \end{jllisting}


\section{FFTW}

The most used package for numerical Fourier transform is probably FFTW\sidenote{\url{https://www.fftw.org/}}. You have to pay attention to the details of the definition. In particular, the prefactors may differ between different packages. In FFTW, the prefactor $1/N$ changes from the forward to the backward transformation, i.e.
%
\begin{equation}
 F_j =   \sum_{k=0}^{N-1} \, f_k \, e^{- k \, j \, 2 \pi i / N } 
\end{equation}
%
and the inverse Fourier transform
%
\begin{equation}
f_k =  \frac{1}{N} \, \sum_{j=0}^{N-1} \, F_j \, e^{+ k \,  j \, 2 \pi i / N } \quad .
\end{equation}
In equations, I (and Butz) use mathematical indices (starting from zero).
Some programming languages count from one (e.g., Julia).

One helpful thing of FFTW is that is supplies also a frequency axis. As mentioned above, first come the positive frequencies, starting from zero to the maximum, then the most negative frequency, again rising until just before zero. 
Depending wether the number of samples $N$ is  even or odd, it is a little bit of a hassle 
to calculate the respective frequencies, but FFTW   does this for us:
\begin{jllisting}
  fftfreq(5) # gives [0.0, 0.2, 0.4, -0.4, -0.2]
  fftfreq(6) # gives [0.0, 0.166, 0.333, -0.5, -0.333, -0.1.66]
\end{jllisting}

\begin{questions}
  \item Try yourself the FFT in a language of your choice. The FFT of, say, $[1 1 1 1]$ should give something like $[4 0 0 0 ]$. 
  \item The inverse FFT is IFFT. Check that it inverts and test how the pre-factors are distributed.
\end{questions}



\subsection{Wrapping \& fftshift}

Now lets look at the Fourier transform of a cosine. We evaluate the cosine at 8 points:
\begin{align}
  x_n = & n \frac{2 \pi}{8} \quad \text{with} \quad n = 0 \dots 7 \\
  f_n = & \cos x_n \\
  F = & \mathcal{FT} (f) \quad .
\end{align}
We find that only $F_1$ and $F_7$ are different from zero and have the same, real value.
Two values must be different from zero because
\begin{equation}
 \cos(x) = \frac{1}{2} \left(e^{i x} + e^{-i x} \right) \quad .
\end{equation}
In general, for real  values $f_n$ we have
\begin{equation}
F_{N-j} = F_j^\star \quad .
\end{equation}



The position of these two non-zero values is a
consequence of the definition of $F_k$: first come all positive
frequencies and then all negative. For a nicer representation it is often better if the frequency zero is not the first element
but in the middle between the positive and negative frequencies.
frequencies. This we get by  \jlinl{fftshift} or backwards by \jlinl{ifftshift}.  

\begin{questions}
  \item Convince yourself that you understand why it is element 1 and 7 that differs from zero in the example above.
  \item Replace the cosine with a sine in this example and
  explain the result.
\end{questions}
  



\section{Sampling theorem}

We need at least two samples per period to describe a function by its
   Fourier coefficients. The frequencies must be
   below the Nyquist frequency $f_\text{Nyquist}$
%
\begin{equation}
f_\text{Nyquist} = \frac{1}{2 \Delta t} \quad .
\end{equation}
%
The \emph{sampling theorem} states that this is then also sufficient, i.e., we do not lose any detail by sampling.
Let $f(t)$ be a bandwidth-limited function, i.e.
$F(\omega)$ is different from zero only in the interval $|\omega| \le \Omega_\text{Nyquist}$. Then the sampling theorem\sidenote{for a proof see \cite{Butz2015}, chap. 4.4} applies and gives 
%
\begin{equation}
f(t) \overset{!}{=} \sum_{k=-\infty}^{\infty} \, f( k \Delta t) \, \text{sinc} \left( \Omega_\text{Nyquist} \cdot [t - k \Delta t] \right) \quad .
\end{equation}
So it is enough to sample $f$ all $\Delta t$. At the
times in between, $f$ is completely described by the (infinitely long) sum of the
neighbouring values times the sinc.

In measurement technology, therefore, all we need to do is ensure, for example by means of an electrical filter, that all the frequencies of a signal are below
$\Omega_\text{Nyquist}$, and then our digital acquisition of the signal will be
is identical to the signal itself.
However, if we sample too infrequently, or if there are higher frequencies present, then these too high frequency components will be reflected at the 
Nyquist frequency and end up at seemingly lower frequencies. This 'aliasing' distorts the signal.
 



\section{Zero padding}


We began with a repeating pattern of numerical values and their
Fourier transform. We always picked the length of the sequence in the examples to match an integer multiple of the period.
But of course, this isn't feasible in reality. We lack accurate knowledge of the signal's duration.
Or sometimes, multiple signals with varying frequencies are important.

The problem is then a truncation error, which leads to artefacts in the
Fourier transform.  Fig. \ref{fig:1_clipping} shows an example. 12 data points of a cosine with period 8 are sampled. The FFT assumes periodic continuation (thick) which is not the 'true' signal (thin). In this case, the FFT of the data is far from a peak at the original frequencies. The real part is even spectrally constant (see below Fig. \ref{fig:1_zeropadding})

\begin{marginfigure}
  \inputtikz{\currfiledir clipping_artefact}
  \caption{Clipping a cosine after 1.5 periods }
  \label{fig:1_clipping}
\end{marginfigure}



The way out is \emph{zero-padding}. Let our actual
measured signal sequence $f(t)$, which we know in the interval $[-T, T]$.
Now we pretend that we measured instead
\begin{equation}
g(t) = f(t) \cdot w(t)
\end{equation}
with the window function $w(t)$
\begin{equation}
w(t) = 1 \quad \text{for} \quad -T < t < T \quad \text{other} = 0 \quad .
\end{equation}
Thus we can `measure' $g(t)$ over arbitrarily long times, because it is
is quasi always zero. But the Fourier transform is
\begin{equation}
G(\omega) = F(\omega) \otimes W(\omega)
\end{equation}
with
\begin{equation}
W(\omega)= 2T \, \frac{\sin \omega T}{\omega T} = 2T \, \text{sinc}( \omega T). \quad .
\end{equation}

 
So we extend our data set on both sides with zeros.
The effect is that we convolve the actual Fourier transform of our
data set with a $\text{sinc}$ whose characteristic width is determined by the actual
measurement duration. The
frequency resolution does not increase. Rather, a kind of
interpolation in Fourier space occurs, which just eliminates the artefacts of the
truncation error.  


We consider the same data set as above, only we 'extend' it to 10 times the length. This means that the clipping error has less
influence and the peak is always at 1 Hz in frequency space. But this does not give
more resolution, of course. Peaks that are close to each other cannot be
separated by zero-padding, only the position of a peak can be be determined better .  

\begin{marginfigure}
  \inputtikz{\currfiledir zeropadding}
  \caption{Zeropadding (line) approaches better the real spectrum (filled symbols) compared to the clipped FT (open symbols).}
  \label{fig:1_zeropadding}
\end{marginfigure}








\section{Windowing}



The oscillations in the spectrum in the last example are still
artefacts. Actually, one would expect two delta functions at $\pm 1$Hz. They are a consequence of the rectangular window $w(t)$, which
leads to the sinc in frequency space. The square-wave window is natural in the sense that we always start and stop measuring. Other window functions\sidenote{\url{https://en.wikipedia.org/wiki/Window_function}}, however, may be better.They differ the width of the peak and the steepness of the slopes. Unfortunately one must trade
one against the other. Interesting parameters are
the width of the central peak in frequency space, measured as a
-3dB bandwidth, as well as the sideband suppression in \sidenote{dB = decibel = $10 \log_10 x$} dB or its
drop in dB/octave.


Typical window functions are (with $|x| = |t/T| < 1/2$ )
\begin{align}
\text{cosine} &  = \cos \pi x \\
\text{triangle} = & 1 - 2 |x| \\
\text{Hanning} = & \cos^2 \pi x \\
 \text{Hamming} = & a + (1-a)\cos^2 \pi x  \\
 \text{Gauss} = & \exp \left( - \frac{1}{2} \frac{x^2}{\sigma^2} \right) \\
\text{Kaiser-Bessel} =  & \frac{I_0(\pi \alpha \sqrt{1-4 x^2})}{I_0(\pi \alpha)} 
\end{align}
with the modified Bessel function $I_0$.




With a window, the measured values are reduced, but the Fourier
transform is smoother, because the transition to the
zero padding becomes smoother. This makes it possible to recognize in the example the peaks at $\pm 1$Hz
even with very few sampled points. 

\begin{marginfigure}
  \inputtikz{\currfiledir zeropadding_window}
  \caption{Zeropadding after windowing (thick) removes the fringes of the un-windowed data (thin) and approaches the true spectrum (solid symbols).}
  \label{fig:1_zeropadding_window}
\end{marginfigure}






We consider as example\sidenote{from \cite{Butz2015}, chapter 3.10} a sum of 6 cosine functions with partly very different amplitudes $A_l$ and frequencies $f_l$:
\begin{align}
  f(t) = & \cos \omega t + 10^{-2} \cos 1.15 \omega t  + 10^{-3} \cos 1.25 \omega t \\
   & + 10^{-3} \cos 2 \omega t  + 10^{-4} \cos 2.75 \omega t + 10^{-5} \cos 3 \omega t \nonumber
\end{align}
We sample 256 data points at intervals of $\Delta t = 1/8$, i.e. only
$8/3 \approx 3$ data points per oscillation of the highest occurring frequency, which is 5 orders of magnitude weaker than the lowest frequency.
Nevertheless, this peak can be found with a suitable window and
zero-padding.  



\begin{marginfigure}
  \inputtikz{\currfiledir example_rect}

  \inputtikz{\currfiledir example_hanning}
  
  \caption{Without windowing (top), only the main signal component is recovered. A Hanning window (bottom) allows to find even signals $10^{-5}$ below the main component.}
  \label{fig:1_example_windowing}
\end{marginfigure}



%--------------------
\printbibliography[segment=\therefsegment,heading=subbibliography]
